{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Regression Techniques"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Multiple linear regression"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset = pd.read_csv('Companies.csv')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "   R&D Spend  Administration  Marketing Spend       State     Profit\n0  165349.20       136897.80        471784.10    New York  192261.83\n1  162597.70       151377.59        443898.53  California  191792.06\n2  153441.51       101145.55        407934.54     Florida  191050.39\n3  144372.41       118671.85        383199.62    New York  182901.99\n4  142107.34        91391.77        366168.42     Florida  166187.94",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R&amp;D Spend</th>\n      <th>Administration</th>\n      <th>Marketing Spend</th>\n      <th>State</th>\n      <th>Profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165349.20</td>\n      <td>136897.80</td>\n      <td>471784.10</td>\n      <td>New York</td>\n      <td>192261.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162597.70</td>\n      <td>151377.59</td>\n      <td>443898.53</td>\n      <td>California</td>\n      <td>191792.06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>153441.51</td>\n      <td>101145.55</td>\n      <td>407934.54</td>\n      <td>Florida</td>\n      <td>191050.39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>144372.41</td>\n      <td>118671.85</td>\n      <td>383199.62</td>\n      <td>New York</td>\n      <td>182901.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>142107.34</td>\n      <td>91391.77</td>\n      <td>366168.42</td>\n      <td>Florida</td>\n      <td>166187.94</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,4].values",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# Encoding Categorical variable\n\nlabelEncoderObj_X = LabelEncoder()\nX[:,3] = labelEncoderObj_X.fit_transform(X[:,3])    # converts the value into number and then OneHotEncoder will be able to encode it",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# one Hot encoder will convert the State column into 3 dummy columns \nonehotencoder = OneHotEncoder(categorical_features = [3] )\nX = onehotencoder.fit_transform(X).toarray()",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Avoiding the dummy variable trap, dummy variable trap occurs when we have multiple dummy columns\n# for a single column which the regression model gets confused with as the values encoded for the categories is 0 or 1\n# hence to avoid issues in the end result we need to remove one of the dummy e.g. 3 dummy columns then use 2, 2 dummy columns then use 1 \n\n# though the library takes care of this but just an explicit step as precation that we dont get trapped\n# this step can be skipped for this example \nX = X[:,1:]",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "raw",
      "source": "X"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training and testing split\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = regressor.predict(X_test)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "array([103015.20159796, 132582.27760815, 132447.73845175,  71976.09851258,\n       178537.48221056, 116161.24230166,  67851.69209676,  98791.73374687,\n       113969.43533013, 167921.06569551])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Building the optimal model by Backward elimination\n# As per the linear regression formula  y = b0 + b1*x1 + b2*x2 ..... + bn*xn\n# b0 (intercept is not included by default) the constant cannot be used individually in backward elimination process hence we need to introduce x0\n# hence using np.append we will add a new column at the start of the X matrix containg all 1's\n\nimport statsmodels.formula.api as sm\n\n# arr is the 1st column containing 1 will be created and it that we will fit the X matrix using values parameter axis 1\nX = np.append(arr = np.ones((50,1)).astype(int) , values = X, axis = 1)\n",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# X_opt is a new variable which will contain the optimal list of independent variables which influence the profit value (dependent variables)\nX_opt = X[:,[0,1,2,3,4,5]]",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# now to start the elimination we will have to calculate the p value of each dependent variable\n# we set the significance level to 0.05 (5%) any independent variable whose p value is less than 5% will stay in the model\n# and for those whose p value is more that 5% will get eliminated from the X matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# OLS ordinary least square model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "            #const, D1,D2,RD,Admin,Mkt\nX_opt = X[:,[0,1,2,3,4,5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     169.9\nDate:                Mon, 29 Jul 2019   Prob (F-statistic):           1.34e-27\nTime:                        15:52:33   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1063.\nDf Residuals:                      44   BIC:                             1074.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\nx1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\nx2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\nx3             0.8060      0.046     17.369      0.000       0.712       0.900\nx4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\nx5             0.0270      0.017      1.574      0.123      -0.008       0.062\n==============================================================================\nOmnibus:                       14.782   Durbin-Watson:                   1.283\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\nSkew:                          -0.948   Prob(JB):                     2.41e-05\nKurtosis:                       5.572   Cond. No.                     1.45e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.45e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 29 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:52:33</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# At this step we removed index 2 (D2 i.e. x2 in model summary) from X matrix as its p values was 0.990 which is more than SL \n# Repeat this step until you get the independent variable set whose p value is less tha SL endog (dependent variable)\n# exog is the matrix of features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "            #const, D1,RD,Admin,Mkt\nX_opt = X[:,[0,1,3,4,5]]    # just a copy of X matrix of features \nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.946\nMethod:                 Least Squares   F-statistic:                     217.2\nDate:                Mon, 29 Jul 2019   Prob (F-statistic):           8.49e-29\nTime:                        15:56:32   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1061.\nDf Residuals:                      45   BIC:                             1070.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\nx1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\nx2             0.8060      0.046     17.606      0.000       0.714       0.898\nx3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\nx4             0.0270      0.017      1.592      0.118      -0.007       0.061\n==============================================================================\nOmnibus:                       14.758   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\nSkew:                          -0.948   Prob(JB):                     2.53e-05\nKurtosis:                       5.563   Cond. No.                     1.40e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 29 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>8.49e-29</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:56:32</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.011e+04</td> <td> 6647.870</td> <td>    7.537</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 6.35e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>  220.1585</td> <td> 2900.536</td> <td>    0.076</td> <td> 0.940</td> <td>-5621.821</td> <td> 6062.138</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.606</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.523</td> <td> 0.604</td> <td>   -0.131</td> <td>    0.077</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.592</td> <td> 0.118</td> <td>   -0.007</td> <td>    0.061</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.758</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.172</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.53e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.563</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# At this step we removed index 1 (D1 i.e. x1 in model summary) from X matrix as its p values was 0.940 which is more than SL",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "        # const,RD,Admin,Mkt\nX_opt = X[:,[0,3,4,5]] \nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     296.0\nDate:                Mon, 29 Jul 2019   Prob (F-statistic):           4.53e-30\nTime:                        15:58:43   Log-Likelihood:                -525.39\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      46   BIC:                             1066.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\nx1             0.8057      0.045     17.846      0.000       0.715       0.897\nx2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\nx3             0.0272      0.016      1.655      0.105      -0.006       0.060\n==============================================================================\nOmnibus:                       14.838   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\nSkew:                          -0.949   Prob(JB):                     2.21e-05\nKurtosis:                       5.586   Cond. No.                     1.40e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 29 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:58:43</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# At this step we removed index 2 (Admin i.e. x2 in model summary) from X matrix as its p values was 0.602 which is more than SL",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "          # const,RD,Mkt\nX_opt = X[:,[0,3,5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.950\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     450.8\nDate:                Mon, 29 Jul 2019   Prob (F-statistic):           2.16e-31\nTime:                        15:59:46   Log-Likelihood:                -525.54\nNo. Observations:                  50   AIC:                             1057.\nDf Residuals:                      47   BIC:                             1063.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\nx1             0.7966      0.041     19.266      0.000       0.713       0.880\nx2             0.0299      0.016      1.927      0.060      -0.001       0.061\n==============================================================================\nOmnibus:                       14.677   Durbin-Watson:                   1.257\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\nSkew:                          -0.939   Prob(JB):                     2.54e-05\nKurtosis:                       5.575   Cond. No.                     5.32e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.32e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 29 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:59:46</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# At this step we removed index 2 (Mkt i.e. x2 in model summary) from X matrix as its p values was 0.06 which is more than SL",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "            # const,RD\nX_opt = X[:,[0,3]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.947\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     849.8\nDate:                Mon, 29 Jul 2019   Prob (F-statistic):           3.50e-32\nTime:                        16:00:30   Log-Likelihood:                -527.44\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      48   BIC:                             1063.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\nx1             0.8543      0.029     29.151      0.000       0.795       0.913\n==============================================================================\nOmnibus:                       13.727   Durbin-Watson:                   1.116\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\nSkew:                          -0.911   Prob(JB):                     9.44e-05\nKurtosis:                       5.361   Cond. No.                     1.65e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.65e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 29 Jul 2019</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:00:30</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# y = 0.00049 + 0.8543 * RD",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}